# 自然语言处理与大语言模型 (Natural Language Processing and Large Language Models) - 课程作业总结

[cite_start]**机构**: 深圳河套学院 (Shenzhen Loop Area Institute) [cite: 1]  
[cite_start]**时间**: 2025年秋季 (Autumn, 2025) [cite: 2]  
[cite_start]**项目**: 期中与期末项目 - 中英机器翻译 (Midterm and Final Project: Machine Translation between Chinese and English) [cite: 3, 4]

---

## 1. 项目目标 (The Goal)
[cite_start]利用 RNN 和 Transformer 模型实现中英机器翻译，并对比它们的性能和架构差异 [cite: 5]。

---

## 2. 作业具体要求 (Assignment Requirements)

### [cite_start]2.1 基于 RNN 的神经机器翻译 (RNN-based NMT) [cite: 14]
你需要构建并训练一个基于 RNN 的 NMT 模型，包含以下内容：
* [cite_start]**模型 (Model)**: 使用 GRU 或 LSTM，编码器 (Encoder) 和解码器 (Decoder) 均由两层单向层组成 [cite: 15]。
* [cite_start]**注意力机制 (Attention mechanism)**: 实现注意力机制，并研究不同对齐函数（如点积 dot-product、乘性 multiplicative、加性 additive）对模型性能的影响 [cite: 16]。
* [cite_start]**训练策略 (Training policy)**: 比较 Teacher Forcing 和 Free Running 策略的有效性 [cite: 17]。
* [cite_start]**解码策略 (Decoding policy)**: 比较贪婪解码 (greedy) 和集束搜索 (beam-search) 解码策略的有效性 [cite: 18]。

### [cite_start]2.2 基于 Transformer 的神经机器翻译 (Transformer-based NMT) [cite: 20]
你需要构建并训练一个基于 Transformer 的 NMT 模型，包含以下内容：
* [cite_start]**从头构建 (From scratch)**: 使用包含编码器-解码器结构的 Transformer 架构，从头构建并训练中英翻译模型 [cite: 21]。
* [cite_start]**架构消融 (Architectural Ablation)**: 从头训练并比较不同位置编码方案（如绝对 vs. 相对）和归一化方法（如 LayerNorm vs. RMSNorm）的效果 [cite: 22]。
* [cite_start]**超参数敏感性 (Hyperparameter Sensitivity)**: 从头训练并改变批量大小、学习率和模型规模，评估其对翻译性能的影响 [cite: 23]。
* [cite_start]**基于预训练语言模型 (From pretrained language model)**: 微调预训练语言模型（如 T5）以适应神经机器翻译，并与从头训练的模型进行性能对比 [cite: 24]。

### [cite_start]2.3 分析与对比 (Analysis and Comparison) [cite: 26]
对基于 RNN 和 Transformer 的 NMT 模型进行全面对比，维度包括：
* [cite_start]**模型架构**: 如顺序 vs. 并行计算，循环 vs. 自注意力 [cite: 27]。
* [cite_start]**训练效率**: 如训练时间、收敛速度、硬件要求 [cite: 28]。
* [cite_start]**翻译性能**: 如 BLEU 分数、流畅度、充分性 [cite: 29]。
* [cite_start]**可扩展性与泛化**: 如处理长句、低资源场景 [cite: 30]。
* [cite_start]**实际权衡**: 如模型大小、推理延迟、实现难易度 [cite: 31]。

---

## 3. 数据集与预处理 (Datasets & Preprocessing)

### 3.1 数据集描述 (Datasets Description)
* [cite_start]**文件**: 压缩包包含4个 JSONL 文件，分别对应小训练集 (100k)、大训练集 (10k)、验证集 (500) 和测试集 (200) [cite: 41]。
    * *注：PPT原文中标注小训练集为100k，大训练集为10k，请以实际文件大小为准。*
* [cite_start]**格式**: JSONL 文件的每一行包含一对平行句子 [cite: 42]。
* [cite_start]**评估**: 最终模型性能将基于测试集结果进行评估 [cite: 43]。
* [cite_start]**获取方式**: Piazza 平台 [cite: 44]。
* [cite_start]**说明**: 资源有限时可仅使用小训练集中的 10k 句对进行训练 [cite: 45][cite_start]，但鼓励尝试大训练集 [cite: 46]。

### [cite_start]3.2 数据预处理 (Datasets Preprocessing) [cite: 47]
* [cite_start]**数据清洗**: 移除非法字符，过滤罕见词；过滤或截断过长句子 [cite: 49]。
* [cite_start]**分词 (Tokenization)**[cite: 50]:
    * [cite_start]**英语**: 自然空格分隔。可直接使用 NLTK 或 BPE、WordPiece 等统计子词分割方法 [cite: 51, 52]。
    * [cite_start]**中文**: 使用 Jieba（轻量）或 HanLP（高精度）等工具 [cite: 53]。
    * [cite_start]**策略**: 鼓励在基线（Jieba+空格）之外探索更高级的分词策略 [cite: 57]。
* [cite_start]**词表构建**: 基于分词数据构建统计词表，建议过滤低频词 [cite: 54, 55]。
* [cite_start]**词嵌入初始化**: 建议使用预训练词向量初始化并在训练中微调 [cite: 56]。

---

## 4. 评估指标 (Metrics)
* [cite_start]**BLEU-4** [cite: 67]。

---

## [cite_start]5. 提交要求 (Submission Requirements) [cite: 75]

### 5.1 提交内容
1.  [cite_start]**源代码**: 提交至 GitHub 仓库，需包含 checkpoints，并提供名为 `inference.py` 的一键推理脚本以方便测试 [cite: 77]。
2.  **项目报告**:
    * [cite_start]**格式**: PDF，命名为 `ID_name.pdf`（如 `250010001_Zhang San.pdf`）[cite: 78]。
    * [cite_start]**内容**: 模型架构描述、代码实现过程解释、实验结果分析（**评分不基于最终 BLEU 分数**）、可视化分析、个人反思 [cite: 78]。
    * [cite_start]**要求**: 必须在报告首页指定位置明确注明代码仓库 URL [cite: 79]。

### 5.2 提交方式与时间
* [cite_start]**提交平台**: 仅需将项目报告提交至 Piazza [cite: 80]。
* [cite_start]**截止日期 (DDL)**: 12月28日 (December 28th) [cite: 81]。

---

## [cite_start]6. 演示 (Presentation) [cite: 82]

* [cite_start]**时长**: 10分钟展示 + 5分钟问答（每组15分钟）[cite: 84]。
* [cite_start]**分组**: 全班18组（每组约7人），可自由组队或随机分配 [cite: 85]。
* [cite_start]**形式**: 每组一次联合展示，但每位学生必须提交个人项目报告 [cite: 86]。
* [cite_start]**截止日期 (DDL)**: 12月28日 (December 28th) [cite: 87]。

---

## [cite_start]7. 评分标准 (Scoring Criteria) [cite: 95]

| 内容 | 占比 |
| :--- | :--- |
| **RNN-based NMT 实现与实验** | [cite_start]15% [cite: 100] |
| **Transformer-based NMT 实现与实验** | [cite_start]25% [cite: 102] |
| **对比分析与讨论** | [cite_start]5% [cite: 104] |
| **项目报告** | [cite_start]5% [cite: 106] |
| **演示 (Presentation)** | [cite_start]50% [cite: 108] |

---

## [cite_start]8. 参考资料 (Reference) [cite: 116]

* [cite_start]**教程**: Seq2Seq Machine Translation Tutorial (PyTorch) [cite: 118]
* **工具**:
    * [cite_start]Jieba (中文分词): https://github.com/fxsjy/jieba [cite: 120]
    * [cite_start]SentencePiece (英语/多语言): https://github.com/google/sentencepiece [cite: 121]
* **论文**:
    * [cite_start]*Attention is All You Need* (Vaswani et al., 2017) [cite: 123]
    * [cite_start]*Neural Machine Translation by Jointly Learning to Align and Translate* (Dzmitry et al., 2015) [cite: 124]
    * [cite_start]*Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer* (Colin et al., 2020) [cite: 125]
* [cite_start]**预训练权重**: HuggingFace T5 (https://huggingface.co/google-t5/t5-base/tree/main) [cite: 126]